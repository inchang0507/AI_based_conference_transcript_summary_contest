AI 기반 회의 녹취록 요약 경진대회
=============
- 주제 : 문서화된 회의 녹취록에서 핵심 내용을 요약하는 생성요약 AI모델 개발
- 주최 : LG AI Research
- 주관 : DACON


## 1. 패키지 로드 및 데이터 로드
### 1-1 필요 패키지 로드
### 1-2 데이터 로드


## 2. 전처리
- 특수문자 제거         
- 문장 길이 측정 : 1) 인코더 및 디코더 입력 길이 판단 시 사용         
- 특징 학습을 위한 레이블 설정 : 본문 길이, 요약문 길이, 요약문 끝문장의 형태를 대상으로 군집화를 수행하여 해당 군집 번호를 레이블로 사용 --> 해당 레이블을 함께 학습시키고 발생하는 손실값을 합하여 요약에 대한 학습과 문장 특징에 대한 학습을 동시에 진행 --> 특징을 고려하는 보다 안정적인 요약 모델을 생성하는 것이 가능         

### 2-1 텍스트 전처리

### 2-2 군집화를 활용한 문장 특징 레이블 생성

### 2-3 학습 데이터 준비

## 3. 모델 설계
### ※ 모델 특징
- 한 모델에서 요약에 대한 학습과 문장 특징 학습(군집화를 통해 구한 클래스에 대한 분류 학습)을 동시에 진행         
- 각 학습을 통해 구해진 loss를 A + (B/10) 비율로 합산하고 이를 역전파를 통해 업데이트         
- 문장의 특징(context의 길이, 요약문의 길이, 안건)을 함께 학습하기 때문에 출력문의 길이와 형태를 보다 잘 나타내어 ***안정적인 학습***이 가능          

### ※ 세부 조정 사항
- 주어진 학습데이터가 상대적으로 적기 때문에 인코딩 층, 디코딩 층 각각의 최하단 부분과 출력층까지만 가중치 업데이트 진행         
- 2 epoch까지 요약문과 문장 특징을 함께 학습하고, 이후 3번의 epoch 요약문 학습만 진행         
- 학습률 스케줄을 적용하여 처음에는 1e-4로 학습 후 warm ratio 초과시 get_cosine_schedule_with_warmup을 이용하여 학습률 조정         


### 3-1 모델 세부 조정
- KoBART 모델 층 구조         
-- encoding part : 2 ~ 97 / Layer norm 98 ~ 99         

-- decoding part : Embedding, PE 100 ~ 101 / Decoding 층 102 ~ 257 / Layer norm 258~259         
         
-- 총 개수 259 개
         
- 주어진 학습 데이터가 상대적으로 적기 때문에 인코딩 층, 디코딩 층 각각의 최하단 부분과 출력층까지 가중치 업데이트 진행         
- 초기에 1e-4로 학습 후 warm ratio 초과시 get_cosine_schedule_with_warmup을 이용하여 학습률 조정         

### 3-2 학습
                  
## 4. 추론


### 4-1 학습된 모델을 이용한 추론

### 4-2 후처리
- 특수문자 제거         
- 앞 뒤 공백 제거         

### 4-3 출력 요약문 확인

### 4-4 결과 저장



